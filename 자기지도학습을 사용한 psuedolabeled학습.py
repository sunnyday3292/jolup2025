# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab
"""

!pip install opencv-python numpy matplotlib ultralytics

from ultralytics import YOLO
import cv2
import numpy as np
import os

# YOLOv8 모델 로드
model = YOLO("yolov8n.pt")  # 가장 가벼운 모델

from google.colab import files
uploaded = files.upload()

# 업로드된 파일명 추출
video_path = next(iter(uploaded))

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


# --- FM-FTRL 임베딩 + 분류기 모델 정의 ---
class FM_FTRL_Encoder(nn.Module):
    def __init__(self, input_dim, embedding_dim=8, k=4):
        super().__init__()
        self.linear = nn.Linear(input_dim, embedding_dim)
        self.V = nn.Parameter(torch.randn(input_dim, k) * 0.01)
        self.embedding_dim = embedding_dim

    def forward(self, x):
        linear_part = self.linear(x)  # (batch, embedding_dim)
        fm_interactions = 0.5 * torch.sum(
            (x @ self.V) ** 2 - (x ** 2) @ (self.V ** 2),
            dim=1,
            keepdim=True
        )
        fm_interactions_expanded = fm_interactions.expand(-1, self.embedding_dim)
        embedding = linear_part + fm_interactions_expanded
        return embedding  # (batch, embedding_dim)

class FM_FTRL_WithClassifier(nn.Module):
    def __init__(self, input_dim, embedding_dim=8, k=4):
        super().__init__()
        self.encoder = FM_FTRL_Encoder(input_dim, embedding_dim, k)
        self.classifier = nn.Linear(embedding_dim, 1)

    def forward(self, x):
        embedding = self.encoder(x)
        logits = self.classifier(embedding)
        return torch.sigmoid(logits).squeeze(1), embedding  # 행동 확률, 임베딩 반환

# --- 입력 특징 전처리 ---
def preprocess_input(cls_id, conf, estimated_distance, box_area, center_x, center_y, num_classes=10):
    cls_one_hot = np.zeros(num_classes)
    if 0 <= cls_id < num_classes:
        cls_one_hot[cls_id] = 1.0

    conf = np.array([conf])
    dist = np.array([estimated_distance / 100.0])
    box_area = np.array([box_area / 10000.0])
    center_x = np.array([center_x / 1920.0])
    center_y = np.array([center_y / 1080.0])

    features = np.concatenate([cls_one_hot, conf, dist, box_area, center_x, center_y])
    return torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # (1, feature_dim)

# --- 노이즈 추가 ---
def add_noise(x, noise_level=0.1):
    noise = noise_level * torch.randn_like(x)
    return x + noise

def process_video_with_frame_actions(video_path, yolo_model, model_fm, optimizer,
                                     camera_focal_length_px=700, real_object_height_m=1.7,
                                     device='cpu', save_output=True):

    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    if save_output:
        os.makedirs("output_frames", exist_ok=True)

    model_fm.to(device)
    model_fm.train()

    loss_fn = nn.MSELoss()
    actions_per_frame = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        results = yolo_model(frame)[0]
        h, w, _ = frame.shape

        frame_actions = []

        for box in results.boxes:
            cls_id = int(box.cls[0].item())
            class_name = results.names[cls_id]
            conf = float(box.conf[0].item())

            x1, y1, x2, y2 = map(int, box.xyxy[0])
            box_w = x2 - x1
            box_h = y2 - y1
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2

            estimated_distance = (camera_focal_length_px * real_object_height_m) / (box_h + 1e-6)

            print(f"\n[Frame {frame_count}] 클래스={class_name}, 신뢰도={conf:.2f}, 거리={estimated_distance:.2f}m, bbox={box_w*box_h}, 중심=({center_x},{center_y})")

            x_input = preprocess_input(cls_id, conf, estimated_distance, box_w * box_h, center_x, center_y)
            x_input = x_input.to(device)

            # 노이즈 추가
            x_noisy = add_noise(x_input)

            # 인코딩 + 분류
            y_pred, emb_orig = model_fm(x_input)
            _, emb_noisy = model_fm(x_noisy)

            # 자기지도 손실 계산
            loss = loss_fn(emb_noisy, emb_orig.detach())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            action = 1 if y_pred.item() >= 0.5 else 0
            frame_actions.append(action)

            print(f"  → 행동 예측: {'주행' if action == 1 else '멈춤'} (확률={y_pred.item():.3f}), SSL Loss={loss.item():.4f}")

            if save_output:
                color = (0, 255, 0) if action == 1 else (0, 0, 255)
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, f"{class_name} {estimated_distance:.1f}m Act:{action}",
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

        # 프레임 단위 다수결 행동 판단
        if len(frame_actions) == 0:
            final_action = 0
        else:
            final_action = 1 if sum(frame_actions) / len(frame_actions) >= 0.5 else 0
        actions_per_frame.append(final_action)
        print(f"Frame {frame_count} 전체 행동 판단: {'주행' if final_action == 1 else '멈춤'}")

        if save_output:
            cv2.putText(frame, f"Frame Action: {'주행' if final_action == 1 else '멈춤'}",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0,
                        (0, 255, 0) if final_action == 1 else (0, 0, 255), 2)
            cv2.imwrite(f"output_frames/frame_{frame_count:03d}.jpg", frame)

    cap.release()
    return actions_per_frame

input_dim = 10 + 5  # num_classes + conf, dist, box_area, cx, cy
model_fm = FM_FTRL_WithClassifier(input_dim=input_dim, embedding_dim=8, k=4)
optimizer = torch.optim.Adam(model_fm.parameters(), lr=1e-3)

process_video_with_frame_actions(video_path, model, model_fm, optimizer)

