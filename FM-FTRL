class FTRLClassifier:
    def __init__(self, alpha=0.1, beta=1.0, l1=1.0, l2=1.0, n_classes=10):
        self.alpha = alpha
        self.beta = beta
        self.l1 = l1
        self.l2 = l2
        self.n_classes = n_classes
        self.n = {}  # squared gradients
        self.z = {}  # accumulated gradients
        self.w = {}  # weights

    def _indices(self, x):
        return x.nonzero()[0]

    def _get_weight(self, i):
        if i not in self.z:
            self.z[i] = 0.
            self.n[i] = 0.
        z = self.z[i]
        n = self.n[i]
        if abs(z) <= self.l1:
            return 0.
        sign = -1. if z < 0 else 1.
        return (sign * self.l1 - z) / ((self.beta + np.sqrt(n)) / self.alpha + self.l2)

    def predict_proba(self, x_vec):
        # softmax prediction
        logits = []
        for c in range(self.n_classes):
            w = np.array([self._get_weight((c, i)) for i in range(len(x_vec))])
            logits.append(np.dot(w, x_vec))
        exp_logits = np.exp(logits - np.max(logits))
        return exp_logits / exp_logits.sum()

    def predict(self, x_vec):
        return np.argmax(self.predict_proba(x_vec))

    def update(self, x_vec, y_true):
        probs = self.predict_proba(x_vec)
        for c in range(self.n_classes):
            y = 1.0 if c == y_true else 0.0
            g = (probs[c] - y) * x_vec
            for i in range(len(x_vec)):
                key = (c, i)
                g_i = g[i]
                sigma = (np.sqrt(self.n.get(key, 0) + g_i ** 2) - np.sqrt(self.n.get(key, 0))) / self.alpha
                self.z[key] = self.z.get(key, 0) + g_i - sigma * self._get_weight(key)
                self.n[key] = self.n.get(key, 0) + g_i ** 2

def process_video(video_path, ftrl_model, camera_focal_length_px=700, real_object_height_m=1.7, save_output=True):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    if save_output:
        os.makedirs("output_frames", exist_ok=True)

    # Accuracy 계산용 기록
    predictions_by_class = defaultdict(list)   # cls_id -> list of predicted class
    groundtruth_by_class = defaultdict(list)   # cls_id -> list of actual class

    all_preds = []
    all_gts = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        results = model(frame)[0]
        h, w, _ = frame.shape

        print(f"\n[Frame {frame_count}]")
        for box in results.boxes:
            cls_id = int(box.cls[0].item())
            class_name = results.names[cls_id]
            conf = float(box.conf[0].item())

            # 바운딩 박스 정보
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            box_w = x2 - x1
            box_h = y2 - y1
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2

            # 거리 추정
            estimated_distance = (camera_focal_length_px * real_object_height_m) / (box_h + 1e-6)

            # Feature vector
            x_vec = np.array([center_x, center_y, box_w * box_h, estimated_distance], dtype=np.float32)

            # 예측 및 업데이트
            pred_cls = ftrl_model.predict(x_vec)
            ftrl_model.update(x_vec, cls_id)

            all_preds.append(pred_cls)
            all_gts.append(cls_id)

            predictions_by_class[cls_id].append(pred_cls)
            groundtruth_by_class[cls_id].append(cls_id)

            print(f"  ▶ 예측={pred_cls}, 실제={cls_id}, 클래스={class_name}, 신뢰도={conf:.2f}, "
                  f"거리={estimated_distance:.2f}m, bbox={box_w * box_h}, 중심=({center_x:.0f},{center_y:.0f})")

            if save_output:
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
                cv2.putText(frame, f"{class_name} {estimated_distance:.1f}m",
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)

        if save_output:
            cv2.imwrite(f"output_frames/frame_{frame_count:03d}.jpg", frame)

    cap.release()
 ### 💡 OCL Metrics 계산 ###
    print("\n\n📊 Online Continual Learning Metrics:")

    # Final Accuracy (FM)
    total_correct = sum([int(p == g) for p, g in zip(all_preds, all_gts)])
    total_count = len(all_gts)
    acc = total_correct / (total_count + 1e-6)
    print(f"FM (Final Accuracy): {acc:.4f}")

    # BWT (Backward Transfer): 이전 class에 대한 성능 변화 (현재보다 나빠졌으면 음수)
    bwt_values = []
    seen_classes = sorted(predictions_by_class.keys())
    for c in seen_classes:
        pred = np.array(predictions_by_class[c])
        gt = np.array(groundtruth_by_class[c])
        acc_at_end = np.mean(pred == gt)
        acc_when_learned = 1.0  # FTRL은 온라인이므로 처음 본 클래스는 예측이 불확실함
        bwt_values.append(acc_at_end - acc_when_learned)
    bwt = np.mean(bwt_values) if bwt_values else 0.0
    print(f"BWT (Backward Transfer): {bwt:.4f}")

    # FWT (Forward Transfer): 현재 학습되지 않은 클래스에 대한 성능 향상 정도 (여기선 적용 어렵지만 형식상 출력)
    fwt = 0.0  # 현재는 모든 클래스가 등장하자마자 학습되므로 측정 불가
    print(f"FWT (Forward Transfer): {fwt:.4f}")

    # Intransigence (IM): 새 클래스에 대한 성능 저하
    im_values = []
    for c in seen_classes:
        pred = np.array(predictions_by_class[c])
        gt = np.array(groundtruth_by_class[c])
        acc_model = np.mean(pred == gt)
        acc_oracle = 1.0  # Ground truth를 항상 맞춘다고 가정
        im_values.append(acc_oracle - acc_model)
    im = np.mean(im_values) if im_values else 0.0
    print(f"IM (Intransigence): {im:.4f}")

# 모델 실행
ftrl_model = FTRLClassifier(n_classes=80)
process_video(video_path,ftrl_model,model)

