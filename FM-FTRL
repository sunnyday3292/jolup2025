class FTRLClassifier:
    def __init__(self, alpha=0.1, beta=1.0, l1=1.0, l2=1.0, n_classes=10):
        self.alpha = alpha
        self.beta = beta
        self.l1 = l1
        self.l2 = l2
        self.n_classes = n_classes
        self.n = {}  # squared gradients
        self.z = {}  # accumulated gradients
        self.w = {}  # weights

    def _indices(self, x):
        return x.nonzero()[0]

    def _get_weight(self, i):
        if i not in self.z:
            self.z[i] = 0.
            self.n[i] = 0.
        z = self.z[i]
        n = self.n[i]
        if abs(z) <= self.l1:
            return 0.
        sign = -1. if z < 0 else 1.
        return (sign * self.l1 - z) / ((self.beta + np.sqrt(n)) / self.alpha + self.l2)

    def predict_proba(self, x_vec):
        # softmax prediction
        logits = []
        for c in range(self.n_classes):
            w = np.array([self._get_weight((c, i)) for i in range(len(x_vec))])
            logits.append(np.dot(w, x_vec))
        exp_logits = np.exp(logits - np.max(logits))
        return exp_logits / exp_logits.sum()

    def predict(self, x_vec):
        return np.argmax(self.predict_proba(x_vec))

    def update(self, x_vec, y_true):
        probs = self.predict_proba(x_vec)
        for c in range(self.n_classes):
            y = 1.0 if c == y_true else 0.0
            g = (probs[c] - y) * x_vec
            for i in range(len(x_vec)):
                key = (c, i)
                g_i = g[i]
                sigma = (np.sqrt(self.n.get(key, 0) + g_i ** 2) - np.sqrt(self.n.get(key, 0))) / self.alpha
                self.z[key] = self.z.get(key, 0) + g_i - sigma * self._get_weight(key)
                self.n[key] = self.n.get(key, 0) + g_i ** 2

def process_video(video_path, ftrl_model, camera_focal_length_px=700, real_object_height_m=1.7, save_output=True):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    if save_output:
        os.makedirs("output_frames", exist_ok=True)

    # Accuracy ê³„ì‚°ìš© ê¸°ë¡
    predictions_by_class = defaultdict(list)   # cls_id -> list of predicted class
    groundtruth_by_class = defaultdict(list)   # cls_id -> list of actual class

    all_preds = []
    all_gts = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        results = model(frame)[0]
        h, w, _ = frame.shape

        print(f"\n[Frame {frame_count}]")
        for box in results.boxes:
            cls_id = int(box.cls[0].item())
            class_name = results.names[cls_id]
            conf = float(box.conf[0].item())

            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            box_w = x2 - x1
            box_h = y2 - y1
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2

            # ê±°ë¦¬ ì¶”ì •
            estimated_distance = (camera_focal_length_px * real_object_height_m) / (box_h + 1e-6)

            # Feature vector
            x_vec = np.array([center_x, center_y, box_w * box_h, estimated_distance], dtype=np.float32)

            # ì˜ˆì¸¡ ë° ì—…ë°ì´íŠ¸
            pred_cls = ftrl_model.predict(x_vec)
            ftrl_model.update(x_vec, cls_id)

            all_preds.append(pred_cls)
            all_gts.append(cls_id)

            predictions_by_class[cls_id].append(pred_cls)
            groundtruth_by_class[cls_id].append(cls_id)

            print(f"  â–¶ ì˜ˆì¸¡={pred_cls}, ì‹¤ì œ={cls_id}, í´ë˜ìŠ¤={class_name}, ì‹ ë¢°ë„={conf:.2f}, "
                  f"ê±°ë¦¬={estimated_distance:.2f}m, bbox={box_w * box_h}, ì¤‘ì‹¬=({center_x:.0f},{center_y:.0f})")

            if save_output:
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
                cv2.putText(frame, f"{class_name} {estimated_distance:.1f}m",
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)

        if save_output:
            cv2.imwrite(f"output_frames/frame_{frame_count:03d}.jpg", frame)

    cap.release()
 ### ğŸ’¡ OCL Metrics ê³„ì‚° ###
    print("\n\nğŸ“Š Online Continual Learning Metrics:")

    # Final Accuracy (FM)
    total_correct = sum([int(p == g) for p, g in zip(all_preds, all_gts)])
    total_count = len(all_gts)
    acc = total_correct / (total_count + 1e-6)
    print(f"FM (Final Accuracy): {acc:.4f}")

    # BWT (Backward Transfer): ì´ì „ classì— ëŒ€í•œ ì„±ëŠ¥ ë³€í™” (í˜„ì¬ë³´ë‹¤ ë‚˜ë¹ ì¡Œìœ¼ë©´ ìŒìˆ˜)
    bwt_values = []
    seen_classes = sorted(predictions_by_class.keys())
    for c in seen_classes:
        pred = np.array(predictions_by_class[c])
        gt = np.array(groundtruth_by_class[c])
        acc_at_end = np.mean(pred == gt)
        acc_when_learned = 1.0  # FTRLì€ ì˜¨ë¼ì¸ì´ë¯€ë¡œ ì²˜ìŒ ë³¸ í´ë˜ìŠ¤ëŠ” ì˜ˆì¸¡ì´ ë¶ˆí™•ì‹¤í•¨
        bwt_values.append(acc_at_end - acc_when_learned)
    bwt = np.mean(bwt_values) if bwt_values else 0.0
    print(f"BWT (Backward Transfer): {bwt:.4f}")

    # FWT (Forward Transfer): í˜„ì¬ í•™ìŠµë˜ì§€ ì•Šì€ í´ë˜ìŠ¤ì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒ ì •ë„ (ì—¬ê¸°ì„  ì ìš© ì–´ë µì§€ë§Œ í˜•ì‹ìƒ ì¶œë ¥)
    fwt = 0.0  # í˜„ì¬ëŠ” ëª¨ë“  í´ë˜ìŠ¤ê°€ ë“±ì¥í•˜ìë§ˆì í•™ìŠµë˜ë¯€ë¡œ ì¸¡ì • ë¶ˆê°€
    print(f"FWT (Forward Transfer): {fwt:.4f}")

    # Intransigence (IM): ìƒˆ í´ë˜ìŠ¤ì— ëŒ€í•œ ì„±ëŠ¥ ì €í•˜
    im_values = []
    for c in seen_classes:
        pred = np.array(predictions_by_class[c])
        gt = np.array(groundtruth_by_class[c])
        acc_model = np.mean(pred == gt)
        acc_oracle = 1.0  # Ground truthë¥¼ í•­ìƒ ë§ì¶˜ë‹¤ê³  ê°€ì •
        im_values.append(acc_oracle - acc_model)
    im = np.mean(im_values) if im_values else 0.0
    print(f"IM (Intransigence): {im:.4f}")

# ëª¨ë¸ ì‹¤í–‰
ftrl_model = FTRLClassifier(n_classes=80)
process_video(video_path,ftrl_model,model)

